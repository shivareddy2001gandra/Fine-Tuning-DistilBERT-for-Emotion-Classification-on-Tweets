{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAg37iLjC5dy"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "import transformers\n",
        "import datasets\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_data = load_dataset(\"dair-ai/emotion\")\n",
        "print(loaded_data)"
      ],
      "metadata": {
        "id": "Un9yViN7EUhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##just\n",
        "print(loaded_data[\"train\"][0])\n",
        "print(loaded_data[\"validation\"][101])\n",
        "print(loaded_data[\"test\"][0])"
      ],
      "metadata": {
        "id": "Yg4ymzn-Ejnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
        "checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "def tokenizenewdataset(loaded_data):\n",
        "  return tokenizer(loaded_data[\"text\"],padding = True,truncation = True)\n",
        "newtokendata = loaded_data.map(tokenizenewdataset)\n",
        "print(newtokendata)"
      ],
      "metadata": {
        "id": "oGgGov2hFX2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n"
      ],
      "metadata": {
        "id": "ec95rAb4HUEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels = 6)"
      ],
      "metadata": {
        "id": "a0pEcW-GIuVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments,Trainer\n",
        "trainingarguments = TrainingArguments(output_dir = \"test-trainer\",\n",
        "                                      per_device_train_batch_size = 8,\n",
        "                                      per_device_eval_batch_size = 8,\n",
        "                                      eval_strategy = \"epoch\",\n",
        "                                      num_train_epochs = 3,\n",
        "                                      weight_decay = 0.01,\n",
        "                                      logging_dir = \"./logs\",\n",
        "                                      learning_rate = 1e-5,\n",
        "                                      load_best_model_at_end = True,\n",
        "                                      save_strategy = \"epoch\",\n",
        "                                      save_total_limit = 3,\n",
        "                                      metric_for_best_model = \"eval_loss\",\n",
        "                                      report_to = \"none\",\n",
        "                                      )"
      ],
      "metadata": {
        "id": "Alme-eCUIcUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model = model,\n",
        "                  args = trainingarguments,\n",
        "                  data_collator = data_collator,\n",
        "                  tokenizer = tokenizer,\n",
        "                  train_dataset = newtokendata[\"train\"],\n",
        "                  eval_dataset =newtokendata[\"validation\"])\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "6YEARohILjmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "M6BxCF1UMvow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(newtokendata[\"test\"])\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "40Jp6_YNM3PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### now converting the logits to label classes we got 6 logits per 1 row because we are classifyingto 6 classes.pick the highest score .predictions return tuples\n",
        "import numpy as np\n",
        "preds = np.argmax(predictions.predictions,axis = 1)"
      ],
      "metadata": {
        "id": "sA8sZBOOZEHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = loaded_data[\"train\"].features[\"label\"].names\n",
        "print(label_names)\n",
        "label_names1 = loaded_data[\"train\"].features[\"label\"]\n",
        "print(type(label_names1))"
      ],
      "metadata": {
        "id": "GnuZxS3pd7lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score,classification_report,f1_score\n",
        "accuracy  = accuracy_score(preds,predictions.label_ids)\n",
        "print(accuracy)\n",
        "label_names = loaded_data[\"train\"].features[\"label\"].names\n",
        "report = classification_report(preds,predictions.label_ids,target_names = label_names)\n",
        "print(report)\n",
        "print(\"F1 Score:\", f1_score(predictions.label_ids, preds,average = \"weighted\"))"
      ],
      "metadata": {
        "id": "c8ItkhambrMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##convertinglabelids to class names\n",
        "# Map predicted label IDs to class names\n",
        "pred_class_names = [label_names[i] for i in preds]\n",
        "print(pred_class_names[:10])\n"
      ],
      "metadata": {
        "id": "fbmCT1_8hCOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}